{
    "title": "Spreadsheet import input",
    "type": "object",
    "schemaVersion": 1,
    "required": ["datasetIds"],
    "properties": {
        "datasetIds": {
            "title": "Dataset IDs",
            "type": "array",
            "description": "Datasets that should be deduplicated and merged",
            "editor": "stringList"
        },
        "fields": {
            "title": "Fields for dedup",
            "type": "array",
            "description": "Fields whose combination should be unique for the item to be considered unique. If none are provided, the actor does not perform deduplication.",
            "editor": "stringList"
        },
        "output": {
            "title": "What to output",
            "type": "string",
            "description": "What will be pushed to the dataset from this actor",
            "editor": "select",
            "default": "unique-items",
            "enum": ["unique-items", "duplicate-items", "nothing"],
            "enumTitles": ["Unique items", "Duplicate items", "Nothing (checking number of duplicates)"]
        },
        "mode": {
            "title": "Mode",
            "type": "string",
            "description": "How the loading and deduplication process will work.",
            "editor": "select",
            "default": "dedup-after-load",
            "enum": ["dedup-after-load", "dedup-as-loading"],
            "enumTitles": ["Dedup after load (high memory use, keeps items ordered as in original datasets)", "Dedup as loading (low memory, doesn't keep items ordered as in original datasets)"]
        },
        "outputTo": {
            "title": "Where to output",
            "type": "string",
            "description": "Either can output to a single dataset or to split data into KV records depending on upload batch size. KV is upload is much faster but data end up in many files.",
            "editor": "select",
            "default": "dataset",
            "enum": ["dataset", "key-value-store"],
            "enumTitles": ["Dataset", "Key-Value Store (split into records)"]
        },
        "outputDatasetId": {
            "title": "Output dataset ID or name (optional)",
            "type": "string",
            "description": "Optionally can push into dataset of your choice. If you provide a dataset name that doesn't exist, a new named dataset will be created.",
            "editor": "textfield"
        },
        "parallelLoads": {
            "title": "Parallel loads",
            "type": "integer",
            "description": "Datasets can be loaded in parallel batches to speed things up if needed.",
            "default": 10,
            "maximum": 100
        },
        "parallelPushes": {
            "title": "Parallel pushes",
            "type": "integer",
            "description": "Deduped data can be pushed in parallel batches to speed things up if needed. Be careful of increasing this as it can introduce performance issues that might lead to duplicates or data corruption.",
            "default": 1,
            "maximum": 20
        },
        "uploadSleepMs": {
            "title": "Dataset upload sleep ms",
            "type": "integer",
            "description": "How long it should wait between each batch when uploading. Useful to not overload Apify API. Only important for dataset upload.",
            "default": 200
        },
        "uploadBatchSize": {
            "title": "Upload batch size",
            "type": "integer",
            "description": "How many items it should upload in one pushData call. Useful to not overload Apify API. Only important for dataset upload.",
            "default": 1000
        },
        "verboseLog": {
            "title": "verbose log",
            "type": "boolean",
            "description": "Good for smaller runs. Large runs might run out of log space.",
            "default": false
        },
        "datasetIdsOfFilterItems": {
            "title": "Dataset IDs for just deduping",
            "type": "array",
            "description": "The items from these datasets will be just used as a dedup filter for the main datasets. These items are loaded first and then the main datasets are compared for uniqueness and pushed.",
            "editor": "stringList"
        },
        "preDedupTransformFunction": {
            "title": "Pre dedup transform function",
            "type": "string",
            "description": "Function to transform items before deduplication is applied. For 'dedup-after-load' mode this is done for all items at once. For 'dedup-as-loading' this is applied to each batch separately.",
            "editor": "javascript",
            "prefill": "async (items, { Apify }) => {\n    return items;\n}"
        },
        "postDedupTransformFunction": {
            "title": "Post dedup transform function",
            "type": "string",
            "description": "Function to transform items after deduplication is applied. For 'dedup-after-load' mode this is done for all items at once. For 'dedup-as-loading' this is applied to each batch separately.",
            "editor": "javascript",
            "prefill": "async (items, { Apify }) => {\n    return items;\n}"
        }
    }
}
