{
    "title": "Spreadsheet import input",
    "type": "object",
    "schemaVersion": 1,
    "required": ["datasetIds", "fields"],
    "properties": {
        "datasetIds": {
            "title": "Dataset IDs",
            "type": "array",
            "description": "Datasets that should be deduplicated and merged",
            "editor": "stringList"
        },
        "fields": {
            "title": "Fields for dedup",
            "type": "array",
            "description": "Fields whose combination should be unique for the item to be considered unique",
            "editor": "stringList"
        },
        "output": {
            "title": "What to output",
            "type": "string",
            "description": "What will be pushed to the dataset from this actor",
            "editor": "select",
            "default": "unique-items",
            "enum": ["unique-items", "duplicate-items", "nothing"],
            "enumTitles": ["Unique items", "Duplicate items", "Nothing (checking number of duplicates)"]
        },
        "mode": {
            "title": "Mode",
            "type": "string",
            "description": "How the loading and deduplication process will work.",
            "editor": "select",
            "default": "dedup-after-load",
            "enum": ["dedup-after-load", "dedup-as-loading"],
            "enumTitles": ["Dedup after load (high memory use, keeps the order of items)", "Dedup as loading (low memory, no order, experimental!!!)"]
        },
        "outputTo": {
            "title": "Where to output",
            "type": "string",
            "description": "Either can output to a single dataset or to split data into KV records depending on upload batch size. KV is upload is much faster but data end up in many files.",
            "editor": "select",
            "default": "dataset",
            "enum": ["dataset", "key-value-store"],
            "enumTitles": ["Dataset", "Key-Value Store (split into records)"]
        },
        "outputDatasetId": {
            "title": "Output dataset ID (optional)",
            "type": "string",
            "description": "Optionally can push into non-default dataset.",
            "editor": "textfield"
        },
        "parallelLoads": {
            "title": "Parallel loads",
            "type": "integer",
            "description": "Datasets can be loaded in parallel batches to speed things up if needed.",
            "default": 10,
            "maximum": 100
        },
        "uploadSleepMs": {
            "title": "Dataset upload sleep ms",
            "type": "integer",
            "description": "How long it should wait between each batch when uploading. Useful to not overload Apify API. Only important for dataset upload.",
            "default": 5000
        },
        "uploadBatchSize": {
            "title": "Upload batch size",
            "type": "integer",
            "description": "How many items it should upload in one pushData call. Useful to not overload Apify API. Only important for dataset upload.",
            "default": 5000
        },
        "datasetIdsOfFilterItems": {
            "title": "Dataset IDs for just deduping",
            "type": "array",
            "description": "The items from these datasets will be just used as a dedup filter for the main datasets. These items are loaded first and then the main datasets are compared for uniqueness and pushed.",
            "editor": "stringList"
        },
        "preDedupTransformFunction": {
            "title": "Pre dedup transform function",
            "type": "string",
            "description": "Function to transform items before deduplication is applied. For 'dedup-after-load' mode this is done for all items at once. For 'dedup-as-loading' this is applied to each batch separately.",
            "editor": "javascript",
            "prefill": "(items) => {\n    return items;\n}"
        },
        "postDedupTransformFunction": {
            "title": "Post dedup transform function",
            "type": "string",
            "description": "Function to transform items after deduplication is applied. For 'dedup-after-load' mode this is done for all items at once. For 'dedup-as-loading' this is applied to each batch separately.",
            "editor": "javascript",
            "prefill": "(items) => {\n    return items;\n}"
        }
    }
}
